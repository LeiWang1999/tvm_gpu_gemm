import tvm
from tvm.script import tir

@tvm.script.ir_module
class Module:
    @tir.prim_func
    def main(inputs_1: tir.Buffer[(1, 56, 56, 64), "float16"], weight_1: tir.Buffer[(64, 3, 3, 64), "float16"], conv2d_nhwc_1: tir.Buffer[(1, 56, 56, 64), "float16"]) -> None:
        # function attr dict
        tir.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with tir.block("root"):
            tir.reads()
            tir.writes()
            tir.block_attr({"meta_schedule.tensor_core_enabled":"1", "warp_execution":1})
            conv2d_nhwc_reindex_wmma_accumulator_1 = tir.alloc_buffer([3136, 64], dtype="float16", scope="wmma.accumulator")
            PadInput_reindex_shared_dyn_1 = tir.alloc_buffer([3136, 576], dtype="float16", scope="shared.dyn")
            weight_reindex_shared_dyn_1 = tir.alloc_buffer([64, 576], dtype="float16", scope="shared.dyn")
            PadInput_reindex_shared_dyn_wmma_matrix_a_1 = tir.alloc_buffer([3136, 576], dtype="float16", scope="wmma.matrix_a")
            weight_reindex_shared_dyn_wmma_matrix_b_1 = tir.alloc_buffer([64, 576], dtype="float16", scope="wmma.matrix_b")
            for ax0_0_ax1_0_0_ax2_0_0_fused in tir.thread_binding(49, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
                for ax0_1_ax1_0_1_ax2_0_1_fused in tir.thread_binding(1, thread="blockIdx.x"):
                    for ax0_2_ax1_0_2_ax2_0_2_fused in tir.thread_binding(4, thread="threadIdx.y"):
                        for ax1_0_3_init, ax2_0_3_init, ax1_0_4_init, ax2_0_4_init in tir.grid(1, 1, 2, 2):
                            with tir.block("conv2d_nhwc_o_init"):
                                v0 = tir.axis.spatial(1, 0)
                                v1_o = tir.axis.spatial(196, ((ax0_0_ax1_0_0_ax2_0_0_fused % 49 + 0) * 2 + ax0_2_ax1_0_2_ax2_0_2_fused % 4 // 2 + ax1_0_3_init) * 2 + ax1_0_4_init)
                                v2_o = tir.axis.spatial(4, ((0 + 0) * 2 + ax0_2_ax1_0_2_ax2_0_2_fused % 2 + ax2_0_3_init) * 2 + ax2_0_4_init)
                                tir.reads()
                                tir.writes(conv2d_nhwc_reindex_wmma_accumulator_1[v1_o * 16 : v1_o * 16 + 16, v2_o * 16 : v2_o * 16 + 16])
                                tir.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32})
                                for ax1_1_0, ax2_1_0 in tir.grid(1, 1):
                                    with tir.block("conv2d_nhwc_init_o"):
                                        v1_init_o = tir.axis.spatial(1, 0)
                                        v2_init_o = tir.axis.spatial(1, 0)
                                        tir.reads()
                                        tir.writes(conv2d_nhwc_reindex_wmma_accumulator_1[v1_o * 16 : v1_o * 16 + 16, v2_o * 16 : v2_o * 16 + 16])
                                        C_2 = tir.match_buffer(conv2d_nhwc_reindex_wmma_accumulator_1[v1_o * 16 : v1_o * 16 + 16, v2_o * 16 : v2_o * 16 + 16], [16, 16], dtype="float16", scope="wmma.accumulator", offset_factor=16)
                                        tir.evaluate(tir.tvm_fill_fragment(C_2.data, 16, 16, 16, C_2.elem_offset // 256 + C_2.elem_offset % 256 // 16, tir.float16(0), dtype="handle"))
                        for ax3_0_0 in tir.serial(9, annotations={"software_pipeline_order":[0, 3, 1, 4, 5, 2, 6], "software_pipeline_stage":[0, 0, 0, 0, 0, 1, 1]}):
                            with tir.block("PadInput_reindex_shared.dyn"):
                                v0, v1 = tir.axis.remap("SS", [ax0_0_ax1_0_0_ax2_0_0_fused, ax3_0_0])
                                tir.reads(inputs_1[0, v0 % 49 * 64 // 56 + v1 // 3 - 1 : v0 % 49 * 64 // 56 + (v0 % 7 * 8 + 63) // 56 + v1 // 3, v1 % 3 - 1 : v1 % 3 + 55, 0 : 64])
                                tir.writes(PadInput_reindex_shared_dyn_1[v0 % 49 * 64 : v0 % 49 * 64 + 64, v1 * 64 : v1 * 64 + 64])
                                tir.block_attr({"auto_copy":1, "double_buffer_scope":0, "local_stage":1, "vector_bytes":16})
                                for ax0, ax1 in tir.grid(64, 64):
                                    PadInput_reindex_shared_dyn_1[v0 % 49 * 64 + ax0, v1 * 64 + ax1] = tir.if_then_else(1 <= (v1 * 64 + ax1) // 192 + (v0 % 49 * 64 + ax0) // 56 and (v1 * 64 + ax1) // 192 + (v0 % 49 * 64 + ax0) // 56 < 57 and 1 <= (v1 * 64 + ax1) % 192 // 64 + (v0 % 49 * 64 + ax0) % 56 and (v1 * 64 + ax1) % 192 // 64 + (v0 % 49 * 64 + ax0) % 56 < 57, inputs_1[0, (v1 * 64 + ax1) // 192 + (v0 % 49 * 64 + ax0) // 56 - 1, (v1 * 64 + ax1) % 192 // 64 + (v0 % 49 * 64 + ax0) % 56 - 1, (v1 * 64 + ax1) % 64], tir.float16(0), dtype="float16")
                            with tir.block("weight_reindex_shared.dyn"):
                                v0 = tir.axis.spatial(9, ax3_0_0)
                                tir.reads(weight_1[0 : 64, v0 // 3, v0 % 3, 0 : 64])
                                tir.writes(weight_reindex_shared_dyn_1[0 : 64, v0 * 64 : v0 * 64 + 64])
                                tir.block_attr({"auto_copy":1, "double_buffer_scope":0, "local_stage":1, "vector_bytes":16})
                                for ax0, ax1 in tir.grid(64, 64):
                                    weight_reindex_shared_dyn_1[ax0, v0 * 64 + ax1] = weight_1[ax0, (v0 * 64 + ax1) // 192, (v0 * 64 + ax1) % 192 // 64, (v0 * 64 + ax1) % 64]
                            for ax3_0_1 in tir.serial(1, annotations={"software_pipeline_order":[0, 1, 2], "software_pipeline_stage":[0, 0, 1]}):
                                with tir.block("PadInput_reindex_shared.dyn_wmma.matrix_a"):
                                    v0, v1, v2, v3 = tir.axis.remap("SSSS", [ax0_0_ax1_0_0_ax2_0_0_fused, ax0_2_ax1_0_2_ax2_0_2_fused, ax3_0_0, ax3_0_1])
                                    tir.reads(PadInput_reindex_shared_dyn_1[v0 % 49 * 64 + v1 % 4 // 2 * 32 : v0 % 49 * 64 + v1 % 4 // 2 * 32 + 32, v2 * 64 + v3 * 64 : v2 * 64 + v3 * 64 + 64])
                                    tir.writes(PadInput_reindex_shared_dyn_wmma_matrix_a_1[v0 % 49 * 64 + v1 % 4 // 2 * 32 : v0 % 49 * 64 + v1 % 4 // 2 * 32 + 32, v2 * 64 + v3 * 64 : v2 * 64 + v3 * 64 + 64])
                                    tir.block_attr({"auto_copy":1})
                                    for ax0, ax1 in tir.grid(32, 64):
                                        PadInput_reindex_shared_dyn_wmma_matrix_a_1[v0 % 49 * 64 + v1 % 4 // 2 * 32 + ax0, v2 * 64 + v3 * 64 + ax1] = PadInput_reindex_shared_dyn_1[v0 % 49 * 64 + v1 % 4 // 2 * 32 + ax0, v2 * 64 + v3 * 64 + ax1]
                                with tir.block("weight_reindex_shared.dyn_wmma.matrix_b"):
                                    v0, v1, v2 = tir.axis.remap("SSS", [ax0_2_ax1_0_2_ax2_0_2_fused, ax3_0_0, ax3_0_1])
                                    tir.reads(weight_reindex_shared_dyn_1[v0 % 2 * 32 : v0 % 2 * 32 + 32, v1 * 64 + v2 * 64 : v1 * 64 + v2 * 64 + 64])
                                    tir.writes(weight_reindex_shared_dyn_wmma_matrix_b_1[v0 % 2 * 32 : v0 % 2 * 32 + 32, v1 * 64 + v2 * 64 : v1 * 64 + v2 * 64 + 64])
                                    tir.block_attr({"auto_copy":1})
                                    for ax0, ax1 in tir.grid(32, 64):
                                        weight_reindex_shared_dyn_wmma_matrix_b_1[v0 % 2 * 32 + ax0, v1 * 64 + v2 * 64 + ax1] = weight_reindex_shared_dyn_1[v0 % 2 * 32 + ax0, v1 * 64 + v2 * 64 + ax1]
                                for ax0_3, ax1_0_3, ax2_0_3, ax3_0_2, ax0_4, ax1_0_4, ax2_0_4 in tir.grid(1, 1, 1, 4, 1, 2, 2):
                                    with tir.block("conv2d_nhwc_o_update"):
                                        v0 = tir.axis.spatial(1, 0)
                                        v1_o = tir.axis.spatial(196, ((ax0_0_ax1_0_0_ax2_0_0_fused % 49 + 0) * 2 + ax0_2_ax1_0_2_ax2_0_2_fused % 4 // 2 + ax1_0_3) * 2 + ax1_0_4)
                                        v2_o = tir.axis.spatial(4, ((0 + 0) * 2 + ax0_2_ax1_0_2_ax2_0_2_fused % 2 + ax2_0_3) * 2 + ax2_0_4)
                                        v3_o = tir.axis.reduce(36, (ax3_0_0 + ax3_0_1) * 4 + ax3_0_2)
                                        tir.reads(conv2d_nhwc_reindex_wmma_accumulator_1[v1_o * 16 : v1_o * 16 + 16, v2_o * 16 : v2_o * 16 + 16], PadInput_reindex_shared_dyn_wmma_matrix_a_1[v1_o * 16 : v1_o * 16 + 16, v3_o * 16 : v3_o * 16 + 16], weight_reindex_shared_dyn_wmma_matrix_b_1[v2_o * 16 : v2_o * 16 + 16, v3_o * 16 : v3_o * 16 + 16])
                                        tir.writes(conv2d_nhwc_reindex_wmma_accumulator_1[v1_o * 16 : v1_o * 16 + 16, v2_o * 16 : v2_o * 16 + 16])
                                        tir.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32})
                                        for ax1_1_0, ax2_1_0, ax3_1_0 in tir.grid(1, 1, 1):
                                            with tir.block("conv2d_nhwc_o"):
                                                v1_o_2 = tir.axis.spatial(1, 0)
                                                v2_o_2 = tir.axis.spatial(1, 0)
                                                v3_o_2 = tir.axis.reduce(1, 0)
                                                tir.reads(conv2d_nhwc_reindex_wmma_accumulator_1[v1_o * 16 : v1_o * 16 + 16, v2_o * 16 : v2_o * 16 + 16], PadInput_reindex_shared_dyn_wmma_matrix_a_1[v1_o * 16 : v1_o * 16 + 16, v3_o * 16 : v3_o * 16 + 16], weight_reindex_shared_dyn_wmma_matrix_b_1[v2_o * 16 : v2_o * 16 + 16, v3_o * 16 : v3_o * 16 + 16])
                                                tir.writes(conv2d_nhwc_reindex_wmma_accumulator_1[v1_o * 16 : v1_o * 16 + 16, v2_o * 16 : v2_o * 16 + 16])
                                                A_1 = tir.match_buffer(PadInput_reindex_shared_dyn_wmma_matrix_a_1[v1_o * 16 : v1_o * 16 + 16, v3_o * 16 : v3_o * 16 + 16], [16, 16], dtype="float16", scope="wmma.matrix_a", offset_factor=16)
                                                B_1 = tir.match_buffer(weight_reindex_shared_dyn_wmma_matrix_b_1[v2_o * 16 : v2_o * 16 + 16, v3_o * 16 : v3_o * 16 + 16], [16, 16], dtype="float16", scope="wmma.matrix_b", offset_factor=16)
                                                C_3 = tir.match_buffer(conv2d_nhwc_reindex_wmma_accumulator_1[v1_o * 16 : v1_o * 16 + 16, v2_o * 16 : v2_o * 16 + 16], [16, 16], dtype="float16", scope="wmma.accumulator", offset_factor=16)
                                                tir.evaluate(tir.tvm_mma_sync(C_3.data, C_3.elem_offset // 256 + C_3.elem_offset % 256 // 16, A_1.data, A_1.elem_offset // 256 + A_1.elem_offset % 256 // 16, B_1.data, B_1.elem_offset // 256 + B_1.elem_offset % 256 // 16, C_3.data, C_3.elem_offset // 256 + C_3.elem_offset % 256 // 16, dtype="handle"))
                        with tir.block("conv2d_nhwc_reindex_wmma.accumulator"):
                            v0, v1 = tir.axis.remap("SS", [ax0_0_ax1_0_0_ax2_0_0_fused, ax0_2_ax1_0_2_ax2_0_2_fused])
                            tir.reads(conv2d_nhwc_reindex_wmma_accumulator_1[v0 % 49 * 64 + v1 % 4 // 2 * 32 : v0 % 49 * 64 + v1 % 4 // 2 * 32 + 32, v1 % 2 * 32 : v1 % 2 * 32 + 32])
                            tir.writes(conv2d_nhwc_1[0, (v0 % 49 * 64 + v1 % 4 // 2 * 32) // 56 : (v0 % 49 * 64 + v1 % 4 // 2 * 32) // 56 + ((v1 % 4 // 2 * 32 + v0 % 49 * 8) % 56 + 31) // 56 + 1, 0 : 56, v1 % 2 * 32 : v1 % 2 * 32 + 32])
                            tir.block_attr({"auto_copy":1, "vector_bytes":8})
                            for ax0, ax1 in tir.grid(32, 32):
                                conv2d_nhwc_1[0, (v0 % 49 * 64 + v1 % 4 // 2 * 32 + ax0) // 56, (v0 % 49 * 64 + v1 % 4 // 2 * 32 + ax0) % 56, v1 % 2 * 32 + ax1] = conv2d_nhwc_reindex_wmma_accumulator_1[v0 % 49 * 64 + v1 % 4 // 2 * 32 + ax0, v1 % 2 * 32 + ax1]

ir_module = Module
sch = tvm.tir.Schedule(ir_module, debug_mask="all")
b0 = sch.get_block(name="PadInput", func_name="main")
b1 = sch.get_block(name="conv2d_nhwc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
b3 = sch.reindex(block=b1, buffer=("write", 0))
b4 = sch.reindex(block=b1, buffer=("read", 0))
b5 = sch.reindex(block=b1, buffer=("read", 1))
sch.transform_layout(block=b1, buffer=("write", 0), index_map=lambda h_l, w_l, co_l: (((h_l*56) + w_l), co_l, ))
sch.transform_layout(block=b1, buffer=("read", 0), index_map=lambda h_l, w_l, rh_l, rw_l, rc_l: (((h_l*56) + w_l), (((rh_l*192) + (rw_l*64)) + rc_l), ))
sch.transform_layout(block=b1, buffer=("read", 1), index_map=lambda co_l, rh_l, rw_l, rc_l: (co_l, (((rh_l*192) + (rw_l*64)) + rc_l), ))
sch.transform_block_layout(block=b3, index_map=lambda n_l, h_l, w_l, co_l, rh_l, rw_l, rc_l: (n_l, ((h_l*56) + w_l), co_l, (((rh_l*192) + (rw_l*64)) + rc_l), ))
sch.transform_block_layout(block=b4, index_map=lambda n_l, h_l, w_l, co_l, rh_l, rw_l, rc_l: (n_l, ((h_l*56) + w_l), co_l, (((rh_l*192) + (rw_l*64)) + rc_l), ))
sch.transform_block_layout(block=b5, index_map=lambda n_l, h_l, w_l, co_l, rh_l, rw_l, rc_l: (n_l, ((h_l*56) + w_l), co_l, (((rh_l*192) + (rw_l*64)) + rc_l), ))
sch.transform_block_layout(block=b1, index_map=lambda n_l, h_l, w_l, co_l, rh_l, rw_l, rc_l: (n_l, ((h_l*56) + w_l), co_l, (((rh_l*192) + (rw_l*64)) + rc_l), ))
l6, l7, l8, l9 = sch.get_loops(block=b1)
l10, l11 = sch.split(loop=l9, factors=[36, 16])
l12, l13 = sch.split(loop=l8, factors=[4, 16])
l14, l15 = sch.split(loop=l7, factors=[196, 16])
l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
sch.reorder(l19, l21, l15, l13, l11)
b23 = sch.blockize(loop=l15)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_sync")
sch.annotate(block_or_loop=b23, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_fill")
b24 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b24, ann_key="meta_schedule.tensor_core_enabled", ann_val="1")
b25 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b25, ann_key="warp_execution", ann_val=1)
l26, l27, l28, l29 = sch.get_loops(block=b23)
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l26, n=5, max_innermost_factor=4, decision=[1, 1, 1, 1, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l26, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l27, n=5, max_innermost_factor=4, decision=[49, 1, 2, 1, 2])
l45, l46, l47, l48, l49 = sch.split(loop=l27, factors=[v40, v41, v42, v43, v44])
v50, v51, v52, v53, v54 = sch.sample_perfect_tile(loop=l28, n=5, max_innermost_factor=4, decision=[1, 1, 2, 1, 2])
l55, l56, l57, l58, l59 = sch.split(loop=l28, factors=[v50, v51, v52, v53, v54])
v60, v61, v62 = sch.sample_perfect_tile(loop=l29, n=3, max_innermost_factor=4, decision=[9, 1, 4])
l63, l64, l65 = sch.split(loop=l29, factors=[v60, v61, v62])
sch.reorder(l35, l45, l55, l36, l46, l56, l37, l47, l57, l63, l64, l38, l48, l58, l65, l39, l49, l59)
l66 = sch.fuse(l35, l45, l55)
sch.bind(loop=l66, thread_axis="blockIdx.y")
l67 = sch.fuse(l36, l46, l56)
sch.bind(loop=l67, thread_axis="blockIdx.x")
l68 = sch.fuse(l37, l47, l57)
sch.bind(loop=l68, thread_axis="threadIdx.y")
sch.annotate(block_or_loop=b23, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b23, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b69 = sch.write_at(loop=l68, block=b23, write_buffer_index=0, storage_scope="wmma.accumulator")
sch.reverse_compute_inline(block=b3)
v70 = sch.sample_categorical(candidates=[4, 8, 16], probs=[0.33333333333333331, 0.33333333333333331, 0.33333333333333331], decision=1)
sch.annotate(block_or_loop=b69, ann_key="vector_bytes", ann_val=v70)
b71 = sch.read_at(loop=l63, block=b23, read_buffer_index=0, storage_scope="shared.dyn")
v72 = sch.sample_categorical(candidates=[4, 8, 16], probs=[0.33333333333333331, 0.33333333333333331, 0.33333333333333331], decision=2)
sch.annotate(block_or_loop=b71, ann_key="vector_bytes", ann_val=v72)
sch.annotate(block_or_loop=b71, ann_key="local_stage", ann_val=1)
sch.annotate(block_or_loop=b71, ann_key="double_buffer_scope", ann_val=0)
b73 = sch.read_at(loop=l63, block=b23, read_buffer_index=1, storage_scope="shared.dyn")
v74 = sch.sample_categorical(candidates=[4, 8, 16], probs=[0.33333333333333331, 0.33333333333333331, 0.33333333333333331], decision=2)
sch.annotate(block_or_loop=b73, ann_key="vector_bytes", ann_val=v74)
sch.annotate(block_or_loop=b73, ann_key="local_stage", ann_val=1)
sch.annotate(block_or_loop=b73, ann_key="double_buffer_scope", ann_val=0)
b75 = sch.read_at(loop=l64, block=b23, read_buffer_index=0, storage_scope="wmma.matrix_a")
b76 = sch.read_at(loop=l64, block=b23, read_buffer_index=1, storage_scope="wmma.matrix_b")
sch.compute_inline(block=b4)
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=l64, ann_key="software_pipeline_stage", ann_val=[0, 0, 1])
sch.annotate(block_or_loop=l64, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l63, ann_key="software_pipeline_stage", ann_val=[0, 0, 0, 0, 0, 1, 1])
sch.annotate(block_or_loop=l63, ann_key="software_pipeline_order", ann_val=[0, 3, 1, 4, 5, 2, 6])
sch.compute_inline(block=b0)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
sch.enter_postproc()
b78 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b78, ann_key="meta_schedule.unroll_explicit")
b79, b80, b81, b82, b83, b84 = sch.get_child_blocks(b78)
l85, l86, l87, l88 = sch.get_loops(block=b79)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92 = sch.get_loops(block=b80)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b81)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b82)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b83)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
l115, l116, l117 = sch.get_loops(block=b84)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
b118 = sch.get_block(name="conv2d_nhwc_o", func_name="main")
l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b118)
b131 = sch.decompose_reduction(block=b118, loop=l122)
sch.unannotate(block_or_loop=b131, ann_key="meta_schedule.auto_tensorize")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.auto_tensorize")
b132, = sch.get_child_blocks(b131)
sch.annotate(block_or_loop=b132, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_fill")
b133 = sch.get_block(name="conv2d_nhwc_init", func_name="main")
sch.unannotate(block_or_loop=b133, ann_key="meta_schedule.auto_tensorize")
l134, l135 = sch.get_loops(block=b133)
l136, l137 = sch.split(loop=l135, factors=[1, 16])
l138, l139 = sch.split(loop=l134, factors=[1, 16])
l140, l141, l142, l143 = sch.get_loops(block=b133)
sch.reorder(l142, l139, l137)
sch.tensorize(block_or_loop=l139, tensor_intrin="wmma_fill")
b144 = sch.get_block(name="conv2d_nhwc", func_name="main")
sch.unannotate(block_or_loop=b144, ann_key="meta_schedule.auto_tensorize")
l145, l146, l147 = sch.get_loops(block=b144)
l148, l149 = sch.split(loop=l147, factors=[1, 16])
l150, l151 = sch.split(loop=l146, factors=[1, 16])
l152, l153 = sch.split(loop=l145, factors=[1, 16])
l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b144)
sch.reorder(l156, l158, l153, l151, l149)
sch.tensorize(block_or_loop=l153, tensor_intrin="wmma_sync")
